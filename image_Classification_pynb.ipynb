{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_Classification.pynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPttcwApIcZxLBz5zmK/BxC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HussainPythonista/image_classification_CIFAR_10/blob/main/image_Classification_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR-10\n",
        "\n",
        "- The dataset contains the 60000 images with 32X32 size\n",
        "\n",
        "- 50000 training dataset and 10000 test dataset\n",
        "\n",
        "- It has five 5 training batches and each training batch contains 10000 images\n",
        "\n",
        "- Test batch contains 10000 images"
      ],
      "metadata": {
        "id": "b-18UhttdYgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### - Loading and Visualize the data\n",
        "\n",
        "Here i'm gonna import necessary libraries\n",
        "\n",
        "importing dataset\n",
        "\n",
        "visualize some of the datasets"
      ],
      "metadata": {
        "id": "CP66v1GHqA8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing necessary libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets,layers,models\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "96HVOD5spUMa"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading datasets\n",
        "\n",
        "(X_train,y_train),(X_test,y_test)=datasets.cifar10.load_data()"
      ],
      "metadata": {
        "id": "A8TAuH-wqqgc"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Label for human read classification\n",
        "\n",
        "labels=[\"airplane\",\n",
        "\"automobile\",\n",
        "\"bird\",\n",
        "\"cat\",\n",
        "\"deer\",\n",
        "\"dog\",\n",
        "\"frog\",\n",
        "\"horse\",\n",
        "\"ship\",\n",
        "\"truck\"]"
      ],
      "metadata": {
        "id": "J007D21ztwKk"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the shape of test and train\n",
        "print(\"Shape of train dataset \",X_train.shape)\n",
        "print(\"Shape of class \",y_train.shape)\n",
        "print(\"Shape of test dataset \",X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj-7nvK6rI_o",
        "outputId": "0d057786-d10c-483d-91fc-e15827698f34"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train dataset  (50000, 32, 32, 3)\n",
            "Shape of class  (50000, 1)\n",
            "Shape of test dataset  (10000, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "50000 is number of images\n",
        "\n",
        "32 and 32 is size of an image ie height and width\n",
        "\n",
        "3 is the number of channels RGB"
      ],
      "metadata": {
        "id": "DhnW2Z_oshXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def imageDetails(wantToVisulize,index):\n",
        "  if wantToVisulize==\"Train\":\n",
        "    plt.imshow(X_train[index])\n",
        "    print(\"The given image is \",labels[int(y_train[index])])\n",
        "  if wantToVisulize==\"Test\":\n",
        "    plt.imshow(X_test[index])\n",
        "    print(\"The given image is \",labels[int(y_test[index])])\n",
        "#Index should be less than 500000 if we want to visualize the Xtrain dataset\n",
        "#Index should be less than 100000 if we want to visualize the Xtest dataset\n",
        "index=200\n",
        "wantToVisulize=\"Test\"\n",
        "imageDetails(wantToVisulize,index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "Kv3lkyTtsgiM",
        "outputId": "68ddee5b-e8af-4832-c3cf-2d524e6f6bea"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The given image is  dog\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Checking and changing the y_trian\n",
        "\n",
        "* Mostly the classes or output labels of dataset is in 2D array format,but i want labels in 1D format better change that before do further operation\n",
        "\n"
      ],
      "metadata": {
        "id": "pyHxuOrnwRgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the shape of labels\n",
        "y_train[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjkTG6jkw6lp",
        "outputId": "8c35386a-e62f-4031-e5cc-fb076ae8295e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6],\n",
              "       [9],\n",
              "       [9],\n",
              "       [4],\n",
              "       [1],\n",
              "       [1],\n",
              "       [2],\n",
              "       [7],\n",
              "       [8],\n",
              "       [3]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If u see above we can clearly see that our output is in 2D format but we want one dimension so we need to change that"
      ],
      "metadata": {
        "id": "zM7XSOjDxD9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking shape\n",
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5JnBXkzyNAy",
        "outputId": "3205334f-77ae-4c3f-e296-5ef28e9b94b6"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking other method as well\n",
        "y_train=y_train.reshape(-1,1)\n",
        "print(y_train.shape)\n",
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yfLc4kayVmV",
        "outputId": "ac3ec1a4-04dd-4f3d-809a-0a1e76ac7419"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6],\n",
              "       [9],\n",
              "       [9],\n",
              "       ...,\n",
              "       [9],\n",
              "       [1],\n",
              "       [1]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing shape to 1D array\n",
        "# y_train=y_train.reshape(-1,1000)\n",
        "# print(y_train.shape)\n",
        "# y_train"
      ],
      "metadata": {
        "id": "Zk_CIxxmypTx"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Preprocessing\n",
        "\n",
        "- Normalizing each pixel is most important step in image preprocessing\n",
        "\n",
        "- We simply divide the pixel with/255 thats it\n",
        "\n",
        "- Change the y_train values into categorical"
      ],
      "metadata": {
        "id": "0Bh2JDn0zi4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking pixels of on image\n",
        "X_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm9QrNhUz1wN",
        "outputId": "7af4a51c-3512-4cd7-815f-3d396c4918ca"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 59,  62,  63],\n",
              "        [ 43,  46,  45],\n",
              "        [ 50,  48,  43],\n",
              "        ...,\n",
              "        [158, 132, 108],\n",
              "        [152, 125, 102],\n",
              "        [148, 124, 103]],\n",
              "\n",
              "       [[ 16,  20,  20],\n",
              "        [  0,   0,   0],\n",
              "        [ 18,   8,   0],\n",
              "        ...,\n",
              "        [123,  88,  55],\n",
              "        [119,  83,  50],\n",
              "        [122,  87,  57]],\n",
              "\n",
              "       [[ 25,  24,  21],\n",
              "        [ 16,   7,   0],\n",
              "        [ 49,  27,   8],\n",
              "        ...,\n",
              "        [118,  84,  50],\n",
              "        [120,  84,  50],\n",
              "        [109,  73,  42]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[208, 170,  96],\n",
              "        [201, 153,  34],\n",
              "        [198, 161,  26],\n",
              "        ...,\n",
              "        [160, 133,  70],\n",
              "        [ 56,  31,   7],\n",
              "        [ 53,  34,  20]],\n",
              "\n",
              "       [[180, 139,  96],\n",
              "        [173, 123,  42],\n",
              "        [186, 144,  30],\n",
              "        ...,\n",
              "        [184, 148,  94],\n",
              "        [ 97,  62,  34],\n",
              "        [ 83,  53,  34]],\n",
              "\n",
              "       [[177, 144, 116],\n",
              "        [168, 129,  94],\n",
              "        [179, 142,  87],\n",
              "        ...,\n",
              "        [216, 184, 140],\n",
              "        [151, 118,  84],\n",
              "        [123,  92,  72]]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the same matrix after normalize it\n",
        "X_train[0]/255\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrNiWgWP0A17",
        "outputId": "fd083fdc-eb3f-4daa-a1ba-866701b1c98e"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.23137255, 0.24313725, 0.24705882],\n",
              "        [0.16862745, 0.18039216, 0.17647059],\n",
              "        [0.19607843, 0.18823529, 0.16862745],\n",
              "        ...,\n",
              "        [0.61960784, 0.51764706, 0.42352941],\n",
              "        [0.59607843, 0.49019608, 0.4       ],\n",
              "        [0.58039216, 0.48627451, 0.40392157]],\n",
              "\n",
              "       [[0.0627451 , 0.07843137, 0.07843137],\n",
              "        [0.        , 0.        , 0.        ],\n",
              "        [0.07058824, 0.03137255, 0.        ],\n",
              "        ...,\n",
              "        [0.48235294, 0.34509804, 0.21568627],\n",
              "        [0.46666667, 0.3254902 , 0.19607843],\n",
              "        [0.47843137, 0.34117647, 0.22352941]],\n",
              "\n",
              "       [[0.09803922, 0.09411765, 0.08235294],\n",
              "        [0.0627451 , 0.02745098, 0.        ],\n",
              "        [0.19215686, 0.10588235, 0.03137255],\n",
              "        ...,\n",
              "        [0.4627451 , 0.32941176, 0.19607843],\n",
              "        [0.47058824, 0.32941176, 0.19607843],\n",
              "        [0.42745098, 0.28627451, 0.16470588]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.81568627, 0.66666667, 0.37647059],\n",
              "        [0.78823529, 0.6       , 0.13333333],\n",
              "        [0.77647059, 0.63137255, 0.10196078],\n",
              "        ...,\n",
              "        [0.62745098, 0.52156863, 0.2745098 ],\n",
              "        [0.21960784, 0.12156863, 0.02745098],\n",
              "        [0.20784314, 0.13333333, 0.07843137]],\n",
              "\n",
              "       [[0.70588235, 0.54509804, 0.37647059],\n",
              "        [0.67843137, 0.48235294, 0.16470588],\n",
              "        [0.72941176, 0.56470588, 0.11764706],\n",
              "        ...,\n",
              "        [0.72156863, 0.58039216, 0.36862745],\n",
              "        [0.38039216, 0.24313725, 0.13333333],\n",
              "        [0.3254902 , 0.20784314, 0.13333333]],\n",
              "\n",
              "       [[0.69411765, 0.56470588, 0.45490196],\n",
              "        [0.65882353, 0.50588235, 0.36862745],\n",
              "        [0.70196078, 0.55686275, 0.34117647],\n",
              "        ...,\n",
              "        [0.84705882, 0.72156863, 0.54901961],\n",
              "        [0.59215686, 0.4627451 , 0.32941176],\n",
              "        [0.48235294, 0.36078431, 0.28235294]]])"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It brings value between 0 to 1"
      ],
      "metadata": {
        "id": "6ZsyUHuG0MY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Scaling the Whole X_train Values\n",
        "X_train_scaled=X_train/255"
      ],
      "metadata": {
        "id": "UFp-ivcj2myh"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**\n",
        "\n",
        "Most important thing to know\n",
        "\n",
        "- When we work on the ANN for classification,the most important thing to remember is Loss \n",
        "    \n",
        "    \"If we use categorical_entropy as loss we should convert the labels into one-hot encoding we can convert it into categorical by keras.utils.to_categorical() function\"\n",
        "\n",
        "    \"At the same time if we use sparse_categorical_entropy as loss we shoul reshape the output into 1D array,\""
      ],
      "metadata": {
        "id": "GZgjZjJu6uF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Change the y_train values as categorical which will help us to classify after fed into neural network\n",
        "\n",
        "y_train_categorical=keras.utils.to_categorical(\n",
        "    y_train,num_classes=10,dtype=\"float32\"\n",
        ")"
      ],
      "metadata": {
        "id": "AIJU2dbY4Nhe"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_categorical"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_qtxEW34iE5",
        "outputId": "8392a765-bc59-40ee-fa19-c75a40baf29d"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking other method as well\n",
        "y_train=y_train.reshape(-1,)\n",
        "print(y_train.shape)\n",
        "y_train"
      ],
      "metadata": {
        "id": "blHe8o8M6lCG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fff99898-a82d-4465-8ea7-eb5264cfd168"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6, 9, 9, ..., 9, 1, 1], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model building\n",
        "\n",
        "- Here i'm going to start with Normal ANN Architecture for this classification\n",
        "\n",
        "- And i go further build CNN in same dataset without any regularization\n",
        "\n",
        "- At final i build model with some hyperparameter like BatchNormalization,Dropouts,L2 "
      ],
      "metadata": {
        "id": "EOtjd1jw0Zsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Build ANN model for image classification\n",
        "I create the ANN for image classification with below compile parameters\n",
        "\n",
        "optimizer=\"SGD\" - Stocastic gradient boosting\n",
        "\n",
        "loss=\"sparse_categorical_crossentropy\" -- Because the output labels are in 1,2,3, format,better use sparse_categorical_entropy\n",
        "\n",
        "metrics=[\"accuracy\"] - We estimate it with accuracy value"
      ],
      "metadata": {
        "id": "1TUj88Wq1Cb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Sequential model\n",
        "\n",
        "model=keras.Sequential([\n",
        "                    layers.Flatten(input_shape=(32,32,3)),\n",
        "                    layers.Dense(3000,activation=\"relu\"),\n",
        "                    layers.Dense(1000,activation=\"relu\"),\n",
        "                    layers.Dense(10,activation=\"sigmoid\"),\n",
        "\n",
        "])\n",
        "\n",
        "#Compile the model with loss evaluation metrics and optimizer\n",
        "model.compile(optimizer=\"SGD\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(X_train_scaled,y_train,epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Qq8AYfg0LqU",
        "outputId": "c1283aeb-72ac-4950-d6b2-0472af7bbd66"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.8137 - accuracy: 0.3551\n",
            "Epoch 2/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.6228 - accuracy: 0.4260\n",
            "Epoch 3/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5415 - accuracy: 0.4567\n",
            "Epoch 4/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4818 - accuracy: 0.4762\n",
            "Epoch 5/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4330 - accuracy: 0.4972\n",
            "Epoch 6/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3895 - accuracy: 0.5114\n",
            "Epoch 7/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3508 - accuracy: 0.5264\n",
            "Epoch 8/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3157 - accuracy: 0.5390\n",
            "Epoch 9/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2827 - accuracy: 0.5491\n",
            "Epoch 10/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2544 - accuracy: 0.5615\n",
            "Epoch 11/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2218 - accuracy: 0.5714\n",
            "Epoch 12/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1934 - accuracy: 0.5844\n",
            "Epoch 13/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1660 - accuracy: 0.5937\n",
            "Epoch 14/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1396 - accuracy: 0.6008\n",
            "Epoch 15/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1161 - accuracy: 0.6104\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f43f65bd610>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After fitting the ANN for the dataset we can clearly see that**\n",
        "\n",
        "- There is no reduction in loss which means the back propagation doesn't reduce the loss\n",
        "\n",
        "- Accuracy is also didn't reduce\n",
        "\n",
        "- This is indicate the ANN is not suitable for this particular problem"
      ],
      "metadata": {
        "id": "27ij3OM5VR84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fitting CNN Model\n",
        "\n",
        "Here I'm gonna fit the same dataset with CNN Architecture\n",
        "\n",
        "- The most important thing to remember,when you use softmax as activation function as final layer we normalize the output probablity,if we add the probality value then sum will be 1\n",
        "\n",
        "- The sigmoid activation function will give exactly same output value without normalize it\n",
        "\n"
      ],
      "metadata": {
        "id": "-Rczq5x5Ztyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exampleVal=[33,44]"
      ],
      "metadata": {
        "id": "IINgaqFSlZVH"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Softmax Value\n",
        "softmax1=33/sum(exampleVal)\n",
        "softmax2=44/sum(exampleVal)\n",
        "print(softmax1,softmax2)\n",
        "#Sum will be 1\n",
        "print(\"Softmax Sum\",(softmax1+softmax2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a16cTz1tkpqE",
        "outputId": "6d929fc6-05b2-4e2d-afda-a2888831dbbc"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.42857142857142855 0.5714285714285714\n",
            "Softmax Sum 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sigmoid value\n",
        "\n",
        "print(exampleVal[0],exampleVal[1])\n",
        "#Sum will be 1\n",
        "print(\"Softmax Sum\",(exampleVal[0]+exampleVal[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBjah3BJlXfc",
        "outputId": "84489808-a3bc-4210-fd26-29a95575088b"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33 44\n",
            "Softmax Sum 77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanations for parameters for sequential layer\n",
        "\n",
        "##### Conv2D\n",
        "\n",
        "  - Conv means Convolution \n",
        "  - 2D means 2DImage\n",
        "\n",
        "  - Parameters\n",
        "\n",
        "  In convolution we detect the feature\n",
        "\n",
        "  In first convolution we need to specify input_shape\n",
        "\n",
        "  In Conv2D we only specify the number of filters(feature maps) and filter size,the Convolution by itself find better filter for us and fit it for us too,\n",
        "\n",
        "  and specify activation function as well \n",
        "\n",
        "  we no need to specify any specific filter weights and more\n",
        "\n",
        "**MaxPooling2d**\n",
        "\n",
        "- Here we are pick main feature of particular feature maps\n",
        "\n",
        "- Here is also only specify filter size and stride as well"
      ],
      "metadata": {
        "id": "kZwfKIgdnFnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn=keras.Sequential([\n",
        "                    \n",
        "\n",
        "#                   #CNN Layer\n",
        "                    #First convolution layer\n",
        "                    layers.Conv2D(filters=32,kernel_size=(3,3),activation=\"relu\",input_shape=(32,32,3)),\n",
        "                    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "                    #Second convolution Layer\n",
        "                    layers.Conv2D(filters=64,kernel_size=(3,3),activation=\"relu\"),\n",
        "                    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "                    #We have some matrics value as input and this is come from previous CNN layers we should simply flatten that\n",
        "                    #We dont need specify input size \n",
        "                    #The Network can figure it out automatically\n",
        "                    layers.Flatten(),\n",
        "                    #We dont need that much dense layer\n",
        "                    #CNN would done so many dense work\n",
        "                    layers.Dense(64,activation=\"relu\"),\n",
        "\n",
        "                    #layers.Dense(32,activation=\"relu\"),\n",
        "\n",
        "                    layers.Dense(10,activation=\"softmax\"),\n",
        "\n",
        "])\n"
      ],
      "metadata": {
        "id": "Z86golV1Xc_3"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile the model with some parameters\n",
        "cnn.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "LLuB5uE_qbaB"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fitting the model with train dataset\n",
        "cnn.fit(X_train_scaled,y_train,epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPYq9Bxsq14u",
        "outputId": "f5f203d8-e7ae-4a9b-fbb0-a2546dc677e0"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4887 - accuracy: 0.4627\n",
            "Epoch 2/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1440 - accuracy: 0.5987\n",
            "Epoch 3/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0224 - accuracy: 0.6432\n",
            "Epoch 4/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.9354 - accuracy: 0.6745\n",
            "Epoch 5/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8714 - accuracy: 0.6976\n",
            "Epoch 6/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8216 - accuracy: 0.7155\n",
            "Epoch 7/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7720 - accuracy: 0.7309\n",
            "Epoch 8/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7305 - accuracy: 0.7464\n",
            "Epoch 9/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6888 - accuracy: 0.7613\n",
            "Epoch 10/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6542 - accuracy: 0.7728\n",
            "Epoch 11/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6187 - accuracy: 0.7853\n",
            "Epoch 12/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.5850 - accuracy: 0.7957\n",
            "Epoch 13/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.5519 - accuracy: 0.8078\n",
            "Epoch 14/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.5244 - accuracy: 0.8189\n",
            "Epoch 15/15\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4933 - accuracy: 0.8282\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f43f65cc650>"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate it with Test dataset**\n",
        "\n"
      ],
      "metadata": {
        "id": "eoHAV16At0Ul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HgxcPYoluvky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Preproccsess the data\n",
        "\n",
        "X_test=X_test/255\n",
        "y_test=y_test.reshape(-1,)"
      ],
      "metadata": {
        "id": "WdA4rZT1uvD8"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjIbVCx8vWc-",
        "outputId": "2acbe9ad-6ce2-452d-e926-2bf393ceb922"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 8, 8, ..., 5, 1, 7], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the model\n",
        "cnn.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHTFMnfOvF7w",
        "outputId": "9cff5a13-10b2-408e-d9eb-eb4334cb2765"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 1.0241 - accuracy: 0.6852\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0241456031799316, 0.6851999759674072]"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict the X_test\n",
        "y_pred=cnn.predict(X_test)\n",
        "y_pred[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtK85jW_0QhL",
        "outputId": "d5565382-d8f7-4071-c0af-c36b651f748a"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.37843215e-05, 3.58922625e-05, 5.85895177e-05, 9.67423558e-01,\n",
              "        5.67977200e-04, 3.12172286e-02, 2.92131561e-04, 3.27742855e-05,\n",
              "        1.91554791e-04, 8.65694674e-05],\n",
              "       [2.12097471e-03, 3.83791979e-03, 4.00107268e-07, 6.00489557e-07,\n",
              "        1.71694587e-08, 1.99873629e-08, 2.83060708e-09, 3.34882705e-10,\n",
              "        9.93688524e-01, 3.51533294e-04],\n",
              "       [9.27618071e-02, 3.19596268e-02, 8.42640933e-04, 4.02732519e-03,\n",
              "        7.00600154e-04, 9.82860220e-04, 3.13814875e-04, 9.93129797e-05,\n",
              "        5.33084810e-01, 3.35227162e-01],\n",
              "       [9.74008918e-01, 3.60329977e-05, 5.63234789e-03, 6.53557619e-03,\n",
              "        7.21493876e-03, 2.51686579e-04, 1.10999725e-04, 2.00642709e-04,\n",
              "        5.95289469e-03, 5.59320324e-05],\n",
              "       [1.22760175e-08, 9.98497399e-06, 1.21736992e-03, 1.47315755e-01,\n",
              "        5.64951956e-01, 2.86678784e-02, 2.57817656e-01, 3.67200848e-08,\n",
              "        1.93536616e-05, 2.42859510e-09]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_classes=[np.argmax(element) for element in y_pred]\n",
        "y_classes[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysLbVb4Z0fFu",
        "outputId": "a6de246f-1bfe-4b71-b4d5-e3d33391eef9"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 8, 8, 0, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qsr10Jy206kU",
        "outputId": "9f897029-ebe5-4263-aef9-3b5e01460e00"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 8, 8, 0, 6], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After my first model fit my model predict some images as wrong\n",
        "\n",
        "better handle\n",
        "\n",
        "My model is overfit so i need to handle that as well\n",
        "\n",
        "hyperparameter tuning"
      ],
      "metadata": {
        "id": "5SQNY26Zzghy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model architecture\n",
        "model_drop = keras.Sequential()\n",
        "model_drop.add(keras.layers.Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=X_train.shape[1:]))\n",
        "model_drop.add(keras.layers.Activation('relu'))\n",
        "model_drop.add(keras.layers.Conv2D(32, (3, 3)))\n",
        "model_drop.add(keras.layers.Activation('relu'))\n",
        "model_drop.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model_drop.add(keras.layers.Dropout(0.25))\n",
        "\n",
        "model_drop.add(keras.layers.Conv2D(64, (3, 3), padding='same'))\n",
        "model_drop.add(keras.layers.Activation('relu'))\n",
        "model_drop.add(keras.layers.Conv2D(64, (3, 3)))\n",
        "model_drop.add(keras.layers.Activation('relu'))\n",
        "model_drop.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model_drop.add(keras.layers.Dropout(0.25))\n",
        "\n",
        "model_drop.add(keras.layers.Flatten())\n",
        "model_drop.add(keras.layers.Dense(512))\n",
        "model_drop.add(keras.layers.Activation('relu'))\n",
        "model_drop.add(keras.layers.Dropout(0.5))\n",
        "model_drop.add(keras.layers.Dense(10))\n",
        "model_drop.add(keras.layers.Activation('softmax'))"
      ],
      "metadata": {
        "id": "jTrinWzSFizg"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emop2ZanF49r",
        "outputId": "3a086d5c-9525-494b-d925-e78114f5c60e"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_6 (Flatten)         (None, 3072)              0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 3000)              9219000   \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 1000)              3001000   \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 10)                10010     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,230,010\n",
            "Trainable params: 12,230,010\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_drop.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "HpEZhXk6IFP-"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train \n",
        "model_drop.fit(X_train_scaled,y_train,epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKJmrAoYIKbg",
        "outputId": "4bbd41d1-42f5-4f59-ab35-2acbde308888"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.1198 - accuracy: 0.2096\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.8010 - accuracy: 0.3433\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5861 - accuracy: 0.4206\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4526 - accuracy: 0.4704\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3623 - accuracy: 0.5085\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2934 - accuracy: 0.5347\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2335 - accuracy: 0.5600\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.1801 - accuracy: 0.5799\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.1334 - accuracy: 0.5976\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0880 - accuracy: 0.6131\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0446 - accuracy: 0.6332\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0065 - accuracy: 0.6421\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9679 - accuracy: 0.6590\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9290 - accuracy: 0.6719\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8993 - accuracy: 0.6838\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8654 - accuracy: 0.6937\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8425 - accuracy: 0.7006\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8162 - accuracy: 0.7123\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7915 - accuracy: 0.7219\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7715 - accuracy: 0.7289\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7495 - accuracy: 0.7357\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7324 - accuracy: 0.7426\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7187 - accuracy: 0.7463\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7011 - accuracy: 0.7531\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6857 - accuracy: 0.7584\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6712 - accuracy: 0.7634\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6601 - accuracy: 0.7657\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6405 - accuracy: 0.7749\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6226 - accuracy: 0.7812\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6163 - accuracy: 0.7811\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6030 - accuracy: 0.7886\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5894 - accuracy: 0.7916\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5781 - accuracy: 0.7965\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5704 - accuracy: 0.7969\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5512 - accuracy: 0.8038\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5406 - accuracy: 0.8072\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5309 - accuracy: 0.8113\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5183 - accuracy: 0.8135\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5149 - accuracy: 0.8162\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5032 - accuracy: 0.8212\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4890 - accuracy: 0.8249\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4810 - accuracy: 0.8282\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4754 - accuracy: 0.8302\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4651 - accuracy: 0.8338\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4584 - accuracy: 0.8352\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4490 - accuracy: 0.8398\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4428 - accuracy: 0.8413\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4320 - accuracy: 0.8443\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4222 - accuracy: 0.8501\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4126 - accuracy: 0.8519\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f43f6329150>"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_drop.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O44Xu5jeKvgS",
        "outputId": "0f61ada0-0e9f-4ccc-d603-9186a527344a"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.6087 - accuracy: 0.7952\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6086937189102173, 0.795199990272522]"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model accuracy increase sequentially and model loss is also decrease by some sequential manner, this happen because of dropouts becase it dropped some of the neurons in each layer\n",
        "\n",
        "After evaluate it with the test data accuracy is also increase from 68 to 79.5\n",
        "\n",
        "But we want to note here is that the model accuracy is only atteende 85 after 50 epoches,it happen because of dropouts. "
      ],
      "metadata": {
        "id": "NpYhmeIgKxF7"
      }
    }
  ]
}